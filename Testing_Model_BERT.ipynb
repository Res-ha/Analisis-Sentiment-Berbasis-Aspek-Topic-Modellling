{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJ6jGpPwWP_8","outputId":"5f963505-9c43-4e89-8986-cee72912f0a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_1DTSPPWVwO","outputId":"89eac5e2-1be4-4385-8948-3e3dac4de837"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","import pandas as pd\n","\n","# Mount Google Drive\n","drive.mount('/content/drive/')\n"]},{"cell_type":"markdown","source":["# Testing 1"],"metadata":{"id":"qG7cthnO_-Gu"}},{"cell_type":"markdown","source":["## Model 1"],"metadata":{"id":"M2Q50-HwEtyT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpd3jqZ3WlwT"},"outputs":[],"source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_vader_gojek.csv'\n","vader = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = vader['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(vader['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8T3YwSgoWn7R","outputId":"47cf51b7-f5e0-4398-a2e3-9311d237708b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.2694565971848872\n","Epoch 2/3, Average Loss: 0.15588589655809684\n","Epoch 3/3, Average Loss: 0.10219826291118007\n"]}],"source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"]},{"cell_type":"code","source":["# Inisialisasi vader_score sebelum loop\n","vader_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","vader_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Gojek-Vader:\")\n","vader_score_df = pd.DataFrame(vader_score)\n","print(vader_score_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5o4ayc4o4sQH","outputId":"03d77815-34f6-4ee7-cd72-1cd59be896f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.13\n","Accuracy: 0.95\n","Precision: 0.98\n","Recall: 0.96\n","F1 Score: 0.97\n","\n","Evaluation for SVM Data Gojek-Vader:\n","   Accuracy  Precision    Recall  F1-score\n","0  0.952517   0.976027  0.956376  0.966102\n"]}]},{"cell_type":"markdown","source":["## Predict 1"],"metadata":{"id":"8TECjko4Eq4i"}},{"cell_type":"code","source":["import pickle\n","# Simpan model\n","model_path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Model/distilbert_model.pkl'\n","with open(model_path, 'wb') as f:\n","    pickle.dump(model, f)\n","\n","# Simpan tokenizer\n","tokenizer_path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Model/distilbert_tokenizer.pkl'\n","with open(tokenizer_path, 'wb') as f:\n","    pickle.dump(tokenizer, f)\n"],"metadata":{"id":"Df4fe65k-eq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","import pickle\n","\n","# Load model\n","model_path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Model/distilbert_model.pkl'\n","with open(model_path, 'rb') as f:\n","    model = pickle.load(f)\n","\n","# Load tokenizer\n","tokenizer_path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Model/distilbert_tokenizer.pkl'\n","with open(tokenizer_path, 'rb') as f:\n","    tokenizer = pickle.load(f)\n","\n","# Input teks baru\n","new_text = input(\"\\nMasukkan teks baru: \")\n","\n","# Tokenisasi teks baru\n","inputs = tokenizer(new_text, return_tensors=\"pt\")\n","\n","# Prediksi dengan model\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","predicted_class_id = logits.argmax().item()\n","predicted_label = model.config.id2label[predicted_class_id]\n","\n","print(\"Predicted Sentiment:\", predicted_label)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqPDKvOf_aKW","outputId":"6a44aab0-6808-43f8-841f-7c468ccdeae1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Masukkan teks baru: this place so disgusting\n","Predicted Sentiment: NEGATIVE\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}