{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["TWIkrAci-ONv"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Library**"],"metadata":{"id":"TWIkrAci-ONv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6JFqmQZ30A1","outputId":"9e2ca9e2-d9df-491c-880d-cfbcaf141c88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n","Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n","Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.3.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n","Requirement already satisfied: Wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from Wordcloud) (1.25.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from Wordcloud) (9.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from Wordcloud) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (4.50.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Wordcloud) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->Wordcloud) (1.16.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install nltk\n","!pip install gensim\n","!pip install pyLDAvis\n","!pip install Wordcloud\n","!pip install transformers"]},{"cell_type":"markdown","source":["# **Evaluasi Sentimen**\n","\n","\n"],"metadata":{"id":"u6nYmLCfsgfX"}},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","\n","# Mount Google Drive\n","drive.mount('/content/drive/')\n"],"metadata":{"id":"0OSCHTAaBaeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Evaluasi Model SVM**"],"metadata":{"id":"uwOexqI51p4n"}},{"cell_type":"markdown","source":["### Labeling Vader"],"metadata":{"id":"VivPeG_T1zsU"}},{"cell_type":"code","source":["# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_vader_gojek.csv'\n","vader = pd.read_csv(path)\n","\n","# Split data into features and labels\n","text_content = vader['Content']\n","sentiment_label = vader['Sentiment']\n","\n","# Split data into training and testing sets\n","text_train, text_test, sentiment_train, sentiment_test = train_test_split(text_content, sentiment_label, test_size=0.15, random_state=42)\n","\n","print(\"Jumlah data latih:\", len(text_train))\n","print(\"Jumlah data uji:\", len(text_test))"],"metadata":{"id":"6Q8kr3OC2qb4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2ac983d-c157-4e9c-b37e-d960cb549455"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah data latih: 11931\n","Jumlah data uji: 2106\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","x_train_tfidf = tfidf_vectorizer.fit_transform(text_train)\n","x_test_tfidf = tfidf_vectorizer.transform(text_test)\n","\n","print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n","print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)"],"metadata":{"id":"4i8MD5jp2s2h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7de7357-dde0-40ae-a165-8ee9cfa3f2f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train_tfidf: (11931, 6766)\n","Shape of x_test_tfidf: (2106, 6766)\n"]}]},{"cell_type":"code","source":["# Oversampling with SMOTE\n","smote = SMOTE(sampling_strategy='auto')  # Adjust strategy as needed\n","x_train_resampled, sentiment_train_resampled = smote.fit_resample(x_train_tfidf, sentiment_train)\n","\n","# Print class distributions\n","print(\"Before SMOTE:\")\n","print(sentiment_train.value_counts())\n","print(\"\\nAfter SMOTE:\")\n","print(sentiment_train_resampled.value_counts())\n"],"metadata":{"id":"k9aziVDV2uuJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b770a00-c96e-4ef2-bb7f-7fd950d57a0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before SMOTE:\n","Sentiment\n","Positive    8268\n","Negative    3663\n","Name: count, dtype: int64\n","\n","After SMOTE:\n","Sentiment\n","Positive    8268\n","Negative    8268\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","\n","# Inisialisasi model SVM dengan parameter yang diperbarui\n","svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","# Melatih model dengan data yang telah di-oversample\n","svm_model.fit(x_train_resampled, sentiment_train_resampled)\n","\n","# Melakukan prediksi menggunakan model yang telah dilatih\n","sentiment_pred_svm = svm_model.predict(x_test_tfidf)\n","\n","# Menghitung akurasi model\n","accuracy_svm = accuracy_score(sentiment_test, sentiment_pred_svm)\n","\n","# Membuat laporan klasifikasi tanpa target names\n","classification_rep_svm = classification_report(sentiment_test, sentiment_pred_svm)\n","\n","# Menghitung metrik presisi, recall, dan F1-score\n","precision_svm = precision_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","recall_svm = recall_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","f1_svm = f1_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","\n","# Menampilkan hasil evaluasi\n","print(\"\\nEvaluation Metrics for SVM Model (Gojek-Vader):\")\n","print(f\"Accuracy: {accuracy_svm:.4f}\")\n","print(f\"Precision: {precision_svm:.4f}\")\n","print(f\"Recall: {recall_svm:.4f}\")\n","print(f\"F1-score: {f1_svm:.4f}\")\n","\n","print(\"\\nClassification Report for SVM Model (Gojek-Vader):\\n\", classification_rep_svm)\n"],"metadata":{"id":"2OGVCKAW2wpV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"662f49a3-0982-4049-cae1-06a866a08ede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics for SVM Model (Gojek-Vader):\n","Accuracy: 0.9378\n","Precision: 0.9391\n","Recall: 0.9378\n","F1-score: 0.9382\n","\n","Classification Report for SVM Model (Gojek-Vader):\n","               precision    recall  f1-score   support\n","\n","    Negative       0.87      0.92      0.90       616\n","    Positive       0.97      0.95      0.96      1490\n","\n","    accuracy                           0.94      2106\n","   macro avg       0.92      0.93      0.93      2106\n","weighted avg       0.94      0.94      0.94      2106\n","\n"]}]},{"cell_type":"markdown","source":["### Labeling Blob"],"metadata":{"id":"oHHXB_aJ2A2H"}},{"cell_type":"code","source":["# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_blob_gojek.csv'\n","blob = pd.read_csv(path)\n","\n","# Split data into features and labels\n","text_content = blob['Content']\n","sentiment_label = blob['Sentiment']\n","\n","# Split data into training and testing sets\n","text_train, text_test, sentiment_train, sentiment_test = train_test_split(text_content, sentiment_label, test_size=0.15, random_state=42)\n","\n","print(\"Jumlah data latih:\", len(text_train))\n","print(\"Jumlah data uji:\", len(text_test))"],"metadata":{"id":"TKRY__Hk2-9f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d93e2acf-535a-401d-9f6e-2510c17a41f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah data latih: 11931\n","Jumlah data uji: 2106\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","x_train_tfidf = tfidf_vectorizer.fit_transform(text_train)\n","x_test_tfidf = tfidf_vectorizer.transform(text_test)\n","\n","print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n","print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)"],"metadata":{"id":"hh6MefwS3ATm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9353fbef-a257-4dd6-8989-2331728c9770"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train_tfidf: (11931, 6766)\n","Shape of x_test_tfidf: (2106, 6766)\n"]}]},{"cell_type":"code","source":["# Oversampling with SMOTE\n","smote = SMOTE(sampling_strategy='auto')  # Adjust strategy as needed\n","x_train_resampled, sentiment_train_resampled = smote.fit_resample(x_train_tfidf, sentiment_train)\n","\n","# Print class distributions\n","print(\"Before SMOTE:\")\n","print(sentiment_train.value_counts())\n","print(\"\\nAfter SMOTE:\")\n","print(sentiment_train_resampled.value_counts())\n"],"metadata":{"id":"rG-G-Lup3BZE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"891bab3d-0578-44c4-be78-7d28d7b7a125"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before SMOTE:\n","Sentiment\n","Positive    8982\n","Negative    2949\n","Name: count, dtype: int64\n","\n","After SMOTE:\n","Sentiment\n","Positive    8982\n","Negative    8982\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","\n","# Inisialisasi model SVM dengan parameter yang diperbarui\n","svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","# Melatih model dengan data yang telah di-oversample\n","svm_model.fit(x_train_resampled, sentiment_train_resampled)\n","\n","# Melakukan prediksi menggunakan model yang telah dilatih\n","sentiment_pred_svm = svm_model.predict(x_test_tfidf)\n","\n","# Menghitung akurasi model\n","accuracy_svm = accuracy_score(sentiment_test, sentiment_pred_svm)\n","\n","# Membuat laporan klasifikasi tanpa target names\n","classification_rep_svm = classification_report(sentiment_test, sentiment_pred_svm)\n","\n","# Menghitung metrik presisi, recall, dan F1-score\n","precision_svm = precision_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","recall_svm = recall_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","f1_svm = f1_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","\n","# Menampilkan hasil evaluasi\n","print(\"\\nEvaluation Metrics for SVM Model (Gojek-Blob):\")\n","print(f\"Accuracy: {accuracy_svm:.4f}\")\n","print(f\"Precision: {precision_svm:.4f}\")\n","print(f\"Recall: {recall_svm:.4f}\")\n","print(f\"F1-score: {f1_svm:.4f}\")\n","\n","print(\"\\nClassification Report for SVM Model (Gojek-Blob):\\n\", classification_rep_svm)\n"],"metadata":{"id":"eJTZkfFn3CeL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"abf8ff35-7ac3-495f-80ad-58203b814c03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics for SVM Model (Gojek-Blob):\n","Accuracy: 0.9345\n","Precision: 0.9353\n","Recall: 0.9345\n","F1-score: 0.9348\n","\n","Classification Report for SVM Model (Gojek-Blob):\n","               precision    recall  f1-score   support\n","\n","    Negative       0.85      0.88      0.86       501\n","    Positive       0.96      0.95      0.96      1605\n","\n","    accuracy                           0.93      2106\n","   macro avg       0.91      0.92      0.91      2106\n","weighted avg       0.94      0.93      0.93      2106\n","\n"]}]},{"cell_type":"markdown","source":["### Labeling BERT"],"metadata":{"id":"MXTlWJNc2BM4"}},{"cell_type":"code","source":["# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_bert_gojek.csv'\n","bert = pd.read_csv(path)\n","\n","# Split data into features and labels\n","text_content = bert['Content']\n","sentiment_label = bert['Sentiment']\n","\n","# Split data into training and testing sets\n","text_train, text_test, sentiment_train, sentiment_test = train_test_split(text_content, sentiment_label, test_size=0.15, random_state=42)\n","\n","print(\"Jumlah data latih:\", len(text_train))\n","print(\"Jumlah data uji:\", len(text_test))"],"metadata":{"id":"Rd5B8BQF3I89","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a02988f3-3a28-48e4-b8a9-dd19d5412bbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah data latih: 11931\n","Jumlah data uji: 2106\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","x_train_tfidf = tfidf_vectorizer.fit_transform(text_train)\n","x_test_tfidf = tfidf_vectorizer.transform(text_test)\n","\n","print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n","print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)"],"metadata":{"id":"lFWuCF-r3Kkq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbb033bc-b5a0-4745-b74e-726061b5d480"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train_tfidf: (11931, 6766)\n","Shape of x_test_tfidf: (2106, 6766)\n"]}]},{"cell_type":"code","source":["# Oversampling with SMOTE\n","smote = SMOTE(sampling_strategy='auto')  # Adjust strategy as needed\n","x_train_resampled, sentiment_train_resampled = smote.fit_resample(x_train_tfidf, sentiment_train)\n","\n","# Print class distributions\n","print(\"Before SMOTE:\")\n","print(sentiment_train.value_counts())\n","print(\"\\nAfter SMOTE:\")\n","print(sentiment_train_resampled.value_counts())\n"],"metadata":{"id":"cJIwx15a3LgK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e14d5c76-a642-4762-f323-1351a6b907c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before SMOTE:\n","Sentiment\n","Negative    6670\n","Positive    5261\n","Name: count, dtype: int64\n","\n","After SMOTE:\n","Sentiment\n","Positive    6670\n","Negative    6670\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","\n","# Inisialisasi model SVM dengan parameter yang diperbarui\n","svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","# Melatih model dengan data yang telah di-oversample\n","svm_model.fit(x_train_resampled, sentiment_train_resampled)\n","\n","# Melakukan prediksi menggunakan model yang telah dilatih\n","sentiment_pred_svm = svm_model.predict(x_test_tfidf)\n","\n","# Menghitung akurasi model\n","accuracy_svm = accuracy_score(sentiment_test, sentiment_pred_svm)\n","\n","# Membuat laporan klasifikasi tanpa target names\n","classification_rep_svm = classification_report(sentiment_test, sentiment_pred_svm)\n","\n","# Menghitung metrik presisi, recall, dan F1-score\n","precision_svm = precision_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","recall_svm = recall_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","f1_svm = f1_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","\n","# Menampilkan hasil evaluasi\n","print(\"\\nEvaluation Metrics for SVM Model (Gojek-BERT):\")\n","print(f\"Accuracy: {accuracy_svm:.4f}\")\n","print(f\"Precision: {precision_svm:.4f}\")\n","print(f\"Recall: {recall_svm:.4f}\")\n","print(f\"F1-score: {f1_svm:.4f}\")\n","\n","print(\"\\nClassification Report for SVM Model (Gojek-BERT):\\n\", classification_rep_svm)\n"],"metadata":{"id":"LFXuz26f3Mke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"803347ca-fa36-4cbc-dd4b-10db570ec295"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics for SVM Model (Gojek-BERT):\n","Accuracy: 0.9050\n","Precision: 0.9051\n","Recall: 0.9050\n","F1-score: 0.9050\n","\n","Classification Report for SVM Model (Gojek-BERT):\n","               precision    recall  f1-score   support\n","\n","    Negative       0.92      0.91      0.91      1170\n","    Positive       0.89      0.89      0.89       936\n","\n","    accuracy                           0.91      2106\n","   macro avg       0.90      0.90      0.90      2106\n","weighted avg       0.91      0.91      0.91      2106\n","\n"]}]},{"cell_type":"markdown","source":["## **Evaluasi Model BERT**"],"metadata":{"id":"uU-1lYjptEUi"}},{"cell_type":"markdown","source":["### Labeling Vader\n","\n"],"metadata":{"id":"Ivg7vsZh50tF"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_vader_gojek.csv'\n","vader = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = vader['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(vader['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"Zga40WeWALIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"],"metadata":{"id":"74SCeLxVAL5s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c3f931c-3f97-426e-884d-315711c0aa3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.2704212231506611\n","Epoch 2/3, Average Loss: 0.15551686143108093\n","Epoch 3/3, Average Loss: 0.10081807588883764\n"]}]},{"cell_type":"code","source":["# Inisialisasi vader_score sebelum loop\n","vader_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","vader_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Gojek-Vader:\")\n","vader_score_df = pd.DataFrame(vader_score)\n","print(vader_score_df)\n"],"metadata":{"id":"wxvpE975AMnI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"beaf0740-7113-4a5a-aa5a-cbaeeac13ffd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.14\n","Accuracy: 0.95\n","Precision: 0.96\n","Recall: 0.97\n","F1 Score: 0.96\n","\n","Evaluation for SVM Data Gojek-Vader:\n","   Accuracy  Precision    Recall  F1-score\n","0  0.948718   0.961282  0.966443  0.963855\n"]}]},{"cell_type":"markdown","source":["### Labeling Blob\n","\n"],"metadata":{"id":"hPYYDi7O58T7"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_blob_gojek.csv'\n","blob = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = blob['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(blob['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"ZptYM4nQ6Ebv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"],"metadata":{"id":"63VVo-T26bgp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7eb2cfda-8454-411b-c65a-bfc169d6ec0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.2872546040840028\n","Epoch 2/3, Average Loss: 0.1590120084683633\n","Epoch 3/3, Average Loss: 0.10632302184107316\n"]}]},{"cell_type":"code","source":["# Inisialisasi blob_score sebelum loop\n","blob_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","blob_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Gojek-Blob:\")\n","blob_score_df = pd.DataFrame(blob_score)\n","print(blob_score_df)\n"],"metadata":{"id":"r5LyFwn86bV-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be994d81-494e-4fc8-c35c-1679aa665ab9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.16\n","Accuracy: 0.94\n","Precision: 0.97\n","Recall: 0.95\n","F1 Score: 0.96\n","\n","Evaluation for SVM Data Gojek-Blob:\n","   Accuracy  Precision    Recall  F1-score\n","0  0.937322   0.966434  0.950779  0.958543\n"]}]},{"cell_type":"markdown","source":["### Labeling BERT\n","\n"],"metadata":{"id":"4PGgcrDB6BM1"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_bert_gojek.csv'\n","bert = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = bert['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(bert['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"OX6cv0bn6hbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"],"metadata":{"id":"PBcYgqMx6hTk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"605e27be-4da8-4956-90f0-d7f1ef1889bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.08706372641850932\n","Epoch 2/3, Average Loss: 0.05225087681332049\n","Epoch 3/3, Average Loss: 0.033302600890018505\n"]}]},{"cell_type":"code","source":["# Inisialisasi bert_score sebelum loop\n","bert_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","bert_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Gojek-Bert:\")\n","bert_score_df = pd.DataFrame(bert_score)\n","print(bert_score_df)\n"],"metadata":{"id":"TBgf3JXv6hK0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9ba5f28-5b40-40ac-9c48-222212866687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.08\n","Accuracy: 0.97\n","Precision: 0.98\n","Recall: 0.95\n","F1 Score: 0.96\n","\n","Evaluation for SVM Data Gojek-Bert:\n","   Accuracy  Precision    Recall  F1-score\n","0  0.966762   0.975824  0.948718   0.96208\n"]}]},{"cell_type":"markdown","source":["# **Evaluasi Aspek**"],"metadata":{"id":"mD5QqLctbtwm"}},{"cell_type":"markdown","source":["## **Evaluasi Model SVM**"],"metadata":{"id":"XYTN7IGfb2Ud"}},{"cell_type":"markdown","source":["### Labelling Vader"],"metadata":{"id":"xo8Ctyg2b-bi"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_vader_gojek.csv'\n","vader = pd.read_csv(path)\n","\n","# Hitung nilai unik dari kolom aspek\n","unique_aspects = vader['Aspect'].unique()\n","\n","# Tampilkan nilai unik dari kolom aspek\n","print(\"Data Aspek dan Jumlah Sentimennya:\")\n","for aspect in unique_aspects:\n","    aspect_sentiments = vader[vader['Aspect'] == aspect]['Sentiment'].value_counts()\n","    print(f\"Aspek: {aspect}\")\n","    print(\"Sentimen  Jumlah\")\n","    for sentiment, count in aspect_sentiments.items():\n","        print(f\"{sentiment.ljust(9)} {count}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0lmgrHOcBzG","outputId":"1b4bf516-5813-4e7c-90b1-2669470aa6ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Aspek dan Jumlah Sentimennya:\n","Aspek: Service\n","Sentimen  Jumlah\n","Negative  2177\n","Positive  1669\n","\n","Aspek: Payment\n","Sentimen  Jumlah\n","Positive  1158\n","Negative  1077\n","\n","Aspek: User Experience\n","Sentimen  Jumlah\n","Positive  6931\n","Negative  1025\n","\n"]}]},{"cell_type":"code","source":["# Import modul yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","\n","# Membuat list kosong untuk menyimpan hasil evaluasi\n","vader_scores = []\n","\n","# Iterasi melalui setiap aspek unik\n","for aspect in unique_aspects:\n","    # Filter data berdasarkan aspek\n","    aspect_data = vader[vader['Aspect'] == aspect]\n","\n","    # Bagi data menjadi fitur (X) dan label sentimen (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Bagi data menjadi training dan testing set (85% train,15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Ubah data teks menjadi fitur numerik menggunakan TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # Lakukan SMOTE pada data training saja\n","    smote = SMOTE(sampling_strategy='auto')  # Sesuaikan strategi jika perlu\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n","\n","    # Buat model SVM untuk klasifikasi sentimen\n","    svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","    # Latih model SVM pada data training\n","    svm_model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Prediksi label sentimen pada data testing\n","    y_pred = svm_model.predict(X_test_tfidf)\n","\n","    # Menghitung metrik evaluasi\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    # Menambahkan hasil evaluasi ke dalam list\n","    vader_scores.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","    # Menampilkan laporan klasifikasi untuk aspek tertentu\n","    print(f\"\\nAspek: {aspect}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-score: {f1:.2f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluasi untuk Semua Aspek (Gojek-Vader):\")\n","vader_scores = pd.DataFrame(vader_scores)\n","print(vader_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NNromdVc09t","outputId":"a7ef10b4-f758-4946-d8cd-54bbeac8ff0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Aspek: Service\n","Accuracy: 0.84\n","Precision: 0.84\n","Recall: 0.84\n","F1-score: 0.84\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.88      0.83      0.86       333\n","    Positive       0.79      0.84      0.82       244\n","\n","    accuracy                           0.84       577\n","   macro avg       0.83      0.84      0.84       577\n","weighted avg       0.84      0.84      0.84       577\n","\n","\n","Aspek: Payment\n","Accuracy: 0.85\n","Precision: 0.85\n","Recall: 0.85\n","F1-score: 0.84\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.89      0.76      0.82       153\n","    Positive       0.82      0.92      0.87       183\n","\n","    accuracy                           0.85       336\n","   macro avg       0.85      0.84      0.84       336\n","weighted avg       0.85      0.85      0.84       336\n","\n","\n","Aspek: User Experience\n","Accuracy: 0.95\n","Precision: 0.95\n","Recall: 0.95\n","F1-score: 0.95\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.81      0.81      0.81       150\n","    Positive       0.97      0.97      0.97      1044\n","\n","    accuracy                           0.95      1194\n","   macro avg       0.89      0.89      0.89      1194\n","weighted avg       0.95      0.95      0.95      1194\n","\n","\n","Evaluasi untuk Semua Aspek (Gojek-Vader):\n","            Aspect  Accuracy  Precision    Recall  F1-score\n","0          Service  0.838821   0.841487  0.838821  0.839423\n","1          Payment  0.845238   0.849558  0.845238  0.843631\n","2  User Experience  0.953099   0.953099  0.953099  0.953099\n"]}]},{"cell_type":"markdown","source":["### Labelling Blob"],"metadata":{"id":"8hruc9fpcCIb"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_blob_gojek.csv'\n","blob = pd.read_csv(path)\n","\n","# Hitung nilai unik dari kolom aspek\n","unique_aspects = blob['Aspect'].unique()\n","\n","# Tampilkan nilai unik dari kolom aspek\n","print(\"Data Aspek dan Jumlah Sentimennya:\")\n","for aspect in unique_aspects:\n","    aspect_sentiments = blob[blob['Aspect'] == aspect]['Sentiment'].value_counts()\n","    print(f\"Aspek: {aspect}\")\n","    print(\"Sentimen  Jumlah\")\n","    for sentiment, count in aspect_sentiments.items():\n","        print(f\"{sentiment.ljust(9)} {count}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_dLiByCcDPE","outputId":"4cdfd5a1-7fdb-4207-8b37-6c24cdf08b1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Aspek dan Jumlah Sentimennya:\n","Aspek: Service\n","Sentimen  Jumlah\n","Positive  2072\n","Negative  1774\n","\n","Aspek: Payment\n","Sentimen  Jumlah\n","Positive  1515\n","Negative  720\n","\n","Aspek: User Experience\n","Sentimen  Jumlah\n","Positive  7000\n","Negative  956\n","\n"]}]},{"cell_type":"code","source":["# Import modul yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","\n","# Membuat list kosong untuk menyimpan hasil evaluasi\n","blob_scores = []\n","\n","# Iterasi melalui setiap aspek unik\n","for aspect in unique_aspects:\n","    # Filter data berdasarkan aspek\n","    aspect_data = blob[blob['Aspect'] == aspect]\n","\n","    # Bagi data menjadi fitur (X) dan label sentimen (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Bagi data menjadi training dan testing set (85% train,15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Ubah data teks menjadi fitur numerik menggunakan TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # Lakukan SMOTE pada data training saja\n","    smote = SMOTE(sampling_strategy='auto')  # Sesuaikan strategi jika perlu\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n","\n","    # Buat model SVM untuk klasifikasi sentimen\n","    svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","    # Latih model SVM pada data training\n","    svm_model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Prediksi label sentimen pada data testing\n","    y_pred = svm_model.predict(X_test_tfidf)\n","\n","    # Menghitung metrik evaluasi\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    # Menambahkan hasil evaluasi ke dalam list\n","    blob_scores.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","    # Menampilkan laporan klasifikasi untuk aspek tertentu\n","    print(f\"\\nAspek: {aspect}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-score: {f1:.2f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluasi untuk Semua Aspek (Gojek-Blob):\")\n","blob_scores = pd.DataFrame(blob_scores)\n","print(blob_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sm9p24zQdRkq","outputId":"f00e441e-533d-49ff-f889-f1aca16eecf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Aspek: Service\n","Accuracy: 0.87\n","Precision: 0.87\n","Recall: 0.87\n","F1-score: 0.87\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.87      0.85      0.86       270\n","    Positive       0.87      0.89      0.88       307\n","\n","    accuracy                           0.87       577\n","   macro avg       0.87      0.87      0.87       577\n","weighted avg       0.87      0.87      0.87       577\n","\n","\n","Aspek: Payment\n","Accuracy: 0.88\n","Precision: 0.88\n","Recall: 0.88\n","F1-score: 0.88\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.82      0.81      0.82       110\n","    Positive       0.91      0.92      0.91       226\n","\n","    accuracy                           0.88       336\n","   macro avg       0.87      0.86      0.86       336\n","weighted avg       0.88      0.88      0.88       336\n","\n","\n","Aspek: User Experience\n","Accuracy: 0.97\n","Precision: 0.97\n","Recall: 0.97\n","F1-score: 0.97\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.85      0.87      0.86       141\n","    Positive       0.98      0.98      0.98      1053\n","\n","    accuracy                           0.97      1194\n","   macro avg       0.92      0.93      0.92      1194\n","weighted avg       0.97      0.97      0.97      1194\n","\n","\n","Evaluasi untuk Semua Aspek (Gojek-Blob):\n","            Aspect  Accuracy  Precision    Recall  F1-score\n","0          Service  0.870017   0.870033  0.870017  0.869896\n","1          Payment  0.880952   0.880453  0.880952  0.880669\n","2  User Experience  0.967337   0.967660  0.967337  0.967485\n"]}]},{"cell_type":"markdown","source":["### Labelling BERT"],"metadata":{"id":"vdmaZRQ3cDlj"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_bert_gojek.csv'\n","bert = pd.read_csv(path)\n","\n","# Hitung nilai unik dari kolom aspek\n","unique_aspects = bert['Aspect'].unique()\n","\n","# Tampilkan nilai unik dari kolom aspek\n","print(\"Data Aspek dan Jumlah Sentimennya:\")\n","for aspect in unique_aspects:\n","    aspect_sentiments = bert[bert['Aspect'] == aspect]['Sentiment'].value_counts()\n","    print(f\"Aspek: {aspect}\")\n","    print(\"Sentimen  Jumlah\")\n","    for sentiment, count in aspect_sentiments.items():\n","        print(f\"{sentiment.ljust(9)} {count}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0AVzPgjHb0mJ","outputId":"beb34681-c3fe-4dce-cd2b-14dafc3e2d80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Aspek dan Jumlah Sentimennya:\n","Aspek: Service\n","Sentimen  Jumlah\n","Negative  3338\n","Positive  508\n","\n","Aspek: Payment\n","Sentimen  Jumlah\n","Negative  1959\n","Positive  276\n","\n","Aspek: User Experience\n","Sentimen  Jumlah\n","Positive  5413\n","Negative  2543\n","\n"]}]},{"cell_type":"code","source":["# Import modul yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","\n","# Membuat list kosong untuk menyimpan hasil evaluasi\n","bert_scores = []\n","\n","# Iterasi melalui setiap aspek unik\n","for aspect in unique_aspects:\n","    # Filter data berdasarkan aspek\n","    aspect_data = bert[bert['Aspect'] == aspect]\n","\n","    # Bagi data menjadi fitur (X) dan label sentimen (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Bagi data menjadi training dan testing set (85% train,15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Ubah data teks menjadi fitur numerik menggunakan TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # Lakukan SMOTE pada data training saja\n","    smote = SMOTE(sampling_strategy='auto')  # Sesuaikan strategi jika perlu\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n","\n","    # Buat model SVM untuk klasifikasi sentimen\n","    svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","    # Latih model SVM pada data training\n","    svm_model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Prediksi label sentimen pada data testing\n","    y_pred = svm_model.predict(X_test_tfidf)\n","\n","    # Menghitung metrik evaluasi\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    # Menambahkan hasil evaluasi ke dalam list\n","    bert_scores.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","    # Menampilkan laporan klasifikasi untuk aspek tertentu\n","    print(f\"\\nAspek: {aspect}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-score: {f1:.2f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluasi untuk Semua Aspek (Gojek-Blob):\")\n","bert_scores = pd.DataFrame(bert_scores)\n","print(bert_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8TfHqgEdX7S","outputId":"770d48f9-6d1c-4639-c2fd-5f835b9096bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Aspek: Service\n","Accuracy: 0.88\n","Precision: 0.89\n","Recall: 0.88\n","F1-score: 0.88\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.94      0.91      0.93       501\n","    Positive       0.53      0.64      0.58        76\n","\n","    accuracy                           0.88       577\n","   macro avg       0.74      0.78      0.76       577\n","weighted avg       0.89      0.88      0.88       577\n","\n","\n","Aspek: Payment\n","Accuracy: 0.87\n","Precision: 0.89\n","Recall: 0.87\n","F1-score: 0.88\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.95      0.91      0.93       298\n","    Positive       0.45      0.61      0.52        38\n","\n","    accuracy                           0.87       336\n","   macro avg       0.70      0.76      0.72       336\n","weighted avg       0.89      0.87      0.88       336\n","\n","\n","Aspek: User Experience\n","Accuracy: 0.90\n","Precision: 0.90\n","Recall: 0.90\n","F1-score: 0.90\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.80      0.88      0.84       373\n","    Positive       0.95      0.90      0.92       821\n","\n","    accuracy                           0.90      1194\n","   macro avg       0.87      0.89      0.88      1194\n","weighted avg       0.90      0.90      0.90      1194\n","\n","\n","Evaluasi untuk Semua Aspek (Gojek-Blob):\n","            Aspect  Accuracy  Precision    Recall  F1-score\n","0          Service  0.878683   0.890100  0.878683  0.883476\n","1          Payment  0.872024   0.891229  0.872024  0.879944\n","2  User Experience  0.895310   0.900015  0.895310  0.896637\n"]}]},{"cell_type":"markdown","source":["## Evaluasi Model BERT"],"metadata":{"id":"piQXHL6tcEX6"}},{"cell_type":"markdown","source":["### Labelling Vader"],"metadata":{"id":"LaJWbpnZcViK"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_vader_gojek.csv'\n","vader = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an empty list to store evaluation results\n","vader_score = []\n","\n","# Iterate through each unique aspect\n","for aspect in vader['Aspect'].unique():\n","    # Filter data based on aspect\n","    aspect_data = vader[vader['Aspect'] == aspect]\n","\n","    # Split data into features (X) and sentiment labels (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Split data into training and testing sets (85% train, 15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","    max_length = max_seq_length\n","    tokenized_texts = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","    attention_masks = (padded_texts != 0).float()  # Create attention masks based on padding values\n","\n","    # Trim or pad the sequences to max_length\n","    padded_texts = padded_texts[:, :max_length]\n","    attention_masks = attention_masks[:, :max_length]\n","\n","    # Split dataset into training and validation sets\n","    labels = torch.tensor(y.map({'Positive': 1, 'Negative': 0}).values)\n","    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","        padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n","    )\n","\n","    # Create DataLoader for training and validation sets\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Fine-tune the model\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    for epoch in range(num_train_epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","            true_labels.extend(labels.tolist())\n","    avg_val_loss = val_loss / len(val_dataloader)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","\n","    print(f\"\\nEvaluation for Aspect: {aspect}\")\n","    print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","    # Append evaluation results to the list\n","    vader_score.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","# Display evaluation results for all aspects\n","print(\"\\nEvaluation for All Aspects (Gojek-Vader):\")\n","vader_score = pd.DataFrame(vader_score)\n","print(vader_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78kC1rA5cLMu","outputId":"fea059e0-99dc-4b68-a9b0-b50469adb0fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.5371754296775003\n","Epoch 2/3, Average Loss: 0.32720869468543134\n","Epoch 3/3, Average Loss: 0.25682294864243677\n","\n","Evaluation for Aspect: Service\n","Validation Loss: 0.32\n","Accuracy: 0.88\n","Precision: 0.88\n","Recall: 0.83\n","F1 Score: 0.85\n","Epoch 1/3, Average Loss: 0.31182834034164747\n","Epoch 2/3, Average Loss: 0.19862180023143688\n","Epoch 3/3, Average Loss: 0.13424119763076306\n","\n","Evaluation for Aspect: Payment\n","Validation Loss: 0.27\n","Accuracy: 0.90\n","Precision: 0.91\n","Recall: 0.91\n","F1 Score: 0.91\n","Epoch 1/3, Average Loss: 0.10741050258654451\n","Epoch 2/3, Average Loss: 0.06428731690635378\n","Epoch 3/3, Average Loss: 0.0390756999129838\n","\n","Evaluation for Aspect: User Experience\n","Validation Loss: 0.10\n","Accuracy: 0.96\n","Precision: 0.98\n","Recall: 0.98\n","F1 Score: 0.98\n","\n","Evaluation for All Aspects (Gojek-Vader):\n","            Aspect  Accuracy  Precision    Recall  F1-score\n","0          Service  0.880416   0.878788  0.831967  0.854737\n","1          Payment  0.898810   0.907104  0.907104  0.907104\n","2  User Experience  0.963987   0.978947  0.979885  0.979416\n"]}]},{"cell_type":"markdown","source":["### Labelling Blob"],"metadata":{"id":"XFHCS-8DcWLR"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_blob_gojek.csv'\n","blob = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an empty list to store evaluation results\n","blob_score = []\n","\n","# Iterate through each unique aspect\n","for aspect in blob['Aspect'].unique():\n","    # Filter data based on aspect\n","    aspect_data = blob[blob['Aspect'] == aspect]\n","\n","    # Split data into features (X) and sentiment labels (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Split data into training and testing sets (85% train, 15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","    max_length = max_seq_length\n","    tokenized_texts = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","    attention_masks = (padded_texts != 0).float()  # Create attention masks based on padding values\n","\n","    # Trim or pad the sequences to max_length\n","    padded_texts = padded_texts[:, :max_length]\n","    attention_masks = attention_masks[:, :max_length]\n","\n","    # Split dataset into training and validation sets\n","    labels = torch.tensor(y.map({'Positive': 1, 'Negative': 0}).values)\n","    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","        padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n","    )\n","\n","    # Create DataLoader for training and validation sets\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Fine-tune the model\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    for epoch in range(num_train_epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","            true_labels.extend(labels.tolist())\n","    avg_val_loss = val_loss / len(val_dataloader)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","\n","    print(f\"\\nEvaluation for Aspect: {aspect}\")\n","    print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","    # Append evaluation results to the list\n","    blob_score.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","# Display evaluation results for all aspects\n","print(\"\\nEvaluation for All Aspects (Gojek-Blob):\")\n","blob_score = pd.DataFrame(blob_score)\n","print(blob_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7jvAJDmcWf4","outputId":"039f3922-1e9b-4b1c-d57f-af8c9e66b483"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.5933052700700112\n","Epoch 2/3, Average Loss: 0.35526873634278194\n","Epoch 3/3, Average Loss: 0.27485951643834994\n","\n","Evaluation for Aspect: Service\n","Validation Loss: 0.35\n","Accuracy: 0.85\n","Precision: 0.83\n","Recall: 0.90\n","F1 Score: 0.87\n","Epoch 1/3, Average Loss: 0.3002108777562777\n","Epoch 2/3, Average Loss: 0.23151334077119828\n","Epoch 3/3, Average Loss: 0.16667987958838543\n","\n","Evaluation for Aspect: Payment\n","Validation Loss: 0.29\n","Accuracy: 0.91\n","Precision: 0.93\n","Recall: 0.93\n","F1 Score: 0.93\n","Epoch 1/3, Average Loss: 0.11469044067176445\n","Epoch 2/3, Average Loss: 0.06735256088036552\n","Epoch 3/3, Average Loss: 0.039692604361883946\n","\n","Evaluation for Aspect: User Experience\n","Validation Loss: 0.08\n","Accuracy: 0.97\n","Precision: 0.99\n","Recall: 0.98\n","F1 Score: 0.98\n","\n","Evaluation for All Aspects (Gojek-Blob):\n","            Aspect  Accuracy  Precision    Recall  F1-score\n","0          Service  0.850953   0.831832  0.902280  0.865625\n","1          Payment  0.907738   0.933333  0.929204  0.931264\n","2  User Experience  0.972362   0.985714  0.982906  0.984308\n"]}]},{"cell_type":"markdown","source":["### Labelling BERT"],"metadata":{"id":"33XnSlnRcW87"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Gojek/label_sentiment_bert_gojek.csv'\n","bert = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an empty list to store evaluation results\n","bert_score = []\n","\n","# Iterate through each unique aspect\n","for aspect in bert['Aspect'].unique():\n","    # Filter data based on aspect\n","    aspect_data = bert[bert['Aspect'] == aspect]\n","\n","    # Split data into features (X) and sentiment labels (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Split data into training and testing sets (85% train, 15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","    max_length = max_seq_length\n","    tokenized_texts = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","    attention_masks = (padded_texts != 0).float()  # Create attention masks based on padding values\n","\n","    # Trim or pad the sequences to max_length\n","    padded_texts = padded_texts[:, :max_length]\n","    attention_masks = attention_masks[:, :max_length]\n","\n","    # Split dataset into training and validation sets\n","    labels = torch.tensor(y.map({'Positive': 1, 'Negative': 0}).values)\n","    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","        padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n","    )\n","\n","    # Create DataLoader for training and validation sets\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Fine-tune the model\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    for epoch in range(num_train_epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","            true_labels.extend(labels.tolist())\n","    avg_val_loss = val_loss / len(val_dataloader)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","\n","    print(f\"\\nEvaluation for Aspect: {aspect}\")\n","    print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","    # Append evaluation results to the list\n","    bert_score.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","# Display evaluation results for all aspects\n","print(\"\\nEvaluation for All Aspects (Gojek-BERT):\")\n","bert_score = pd.DataFrame(bert_score)\n","print(bert_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYJGIRDpcXei","outputId":"466cf2c3-d311-46dd-ed69-16ac7bd46f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.08265891637725448\n","Epoch 2/3, Average Loss: 0.04104552206270243\n","Epoch 3/3, Average Loss: 0.019980194950724943\n","\n","Evaluation for Aspect: Service\n","Validation Loss: 0.03\n","Accuracy: 0.99\n","Precision: 0.96\n","Recall: 0.95\n","F1 Score: 0.95\n","Epoch 1/3, Average Loss: 0.11843083501589717\n","Epoch 2/3, Average Loss: 0.048821366671472785\n","Epoch 3/3, Average Loss: 0.02891681042771476\n","\n","Evaluation for Aspect: Payment\n","Validation Loss: 0.06\n","Accuracy: 0.96\n","Precision: 0.86\n","Recall: 0.82\n","F1 Score: 0.84\n","Epoch 1/3, Average Loss: 0.09220043577930345\n","Epoch 2/3, Average Loss: 0.05071335088023433\n","Epoch 3/3, Average Loss: 0.02915637801746719\n","\n","Evaluation for Aspect: User Experience\n","Validation Loss: 0.07\n","Accuracy: 0.98\n","Precision: 0.98\n","Recall: 0.99\n","F1 Score: 0.98\n","\n","Evaluation for All Aspects (Gojek-BERT):\n","            Aspect  Accuracy  Precision    Recall  F1-score\n","0          Service  0.987868   0.960000  0.947368  0.953642\n","1          Payment  0.964286   0.861111  0.815789  0.837838\n","2  User Experience  0.976549   0.979444  0.986602  0.983010\n"]}]}]}