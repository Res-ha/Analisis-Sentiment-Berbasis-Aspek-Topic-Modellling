{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["TWIkrAci-ONv"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Library**"],"metadata":{"id":"TWIkrAci-ONv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6JFqmQZ30A1","outputId":"0a01d150-9b04-4e9a-ffff-ae1f93931361","executionInfo":{"status":"ok","timestamp":1711183878016,"user_tz":-420,"elapsed":5853,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["# **Evaluasi Sentimen**\n","\n","\n"],"metadata":{"id":"u6nYmLCfsgfX"}},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","\n","# Mount Google Drive\n","drive.mount('/content/drive/')\n"],"metadata":{"id":"UhqMEm4X0Sr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Evaluasi Model SVM**"],"metadata":{"id":"uwOexqI51p4n"}},{"cell_type":"markdown","source":["### Labeling Vader"],"metadata":{"id":"VivPeG_T1zsU"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_vader_kaggle.csv'\n","vader = pd.read_csv(path)\n","\n","# Split data into features and labels\n","text_content = vader['Content']\n","sentiment_label = vader['Sentiment']\n","\n","# Split data into training and testing sets\n","text_train, text_test, sentiment_train, sentiment_test = train_test_split(text_content, sentiment_label, test_size=0.15, random_state=42)\n","\n","print(\"Jumlah data latih:\", len(text_train))\n","print(\"Jumlah data uji:\", len(text_test))"],"metadata":{"id":"6Q8kr3OC2qb4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c924423-ccd8-459a-e35f-e2582bf540ff","executionInfo":{"status":"ok","timestamp":1711183878016,"user_tz":-420,"elapsed":14,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah data latih: 13731\n","Jumlah data uji: 2424\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","x_train_tfidf = tfidf_vectorizer.fit_transform(text_train)\n","x_test_tfidf = tfidf_vectorizer.transform(text_test)\n","\n","print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n","print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)"],"metadata":{"id":"4i8MD5jp2s2h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"617387c1-55e3-439d-a7c6-f1a7eea2b24b","executionInfo":{"status":"ok","timestamp":1711183878595,"user_tz":-420,"elapsed":589,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train_tfidf: (13731, 7775)\n","Shape of x_test_tfidf: (2424, 7775)\n"]}]},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","\n","# Oversampling with SMOTE\n","smote = SMOTE(sampling_strategy='auto')  # Adjust strategy as needed\n","x_train_resampled, sentiment_train_resampled = smote.fit_resample(x_train_tfidf, sentiment_train)\n","\n","# Print class distributions\n","print(\"Before SMOTE:\")\n","print(sentiment_train.value_counts())\n","print(\"\\nAfter SMOTE:\")\n","print(sentiment_train_resampled.value_counts())\n"],"metadata":{"id":"k9aziVDV2uuJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1190f954-f93a-46da-8f08-cc0ade9e04ef","executionInfo":{"status":"ok","timestamp":1711183880124,"user_tz":-420,"elapsed":1534,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before SMOTE:\n","Positive    11149\n","Negative     2582\n","Name: Sentiment, dtype: int64\n","\n","After SMOTE:\n","Positive    11149\n","Negative    11149\n","Name: Sentiment, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","\n","# Inisialisasi model SVM dengan parameter yang diperbarui\n","svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","# Melatih model dengan data yang telah di-oversample\n","svm_model.fit(x_train_resampled, sentiment_train_resampled)\n","\n","# Melakukan prediksi menggunakan model yang telah dilatih\n","sentiment_pred_svm = svm_model.predict(x_test_tfidf)\n","\n","# Menghitung akurasi model\n","accuracy_svm = accuracy_score(sentiment_test, sentiment_pred_svm)\n","\n","# Membuat laporan klasifikasi tanpa target names\n","classification_rep_svm = classification_report(sentiment_test, sentiment_pred_svm)\n","\n","# Menghitung metrik presisi, recall, dan F1-score\n","precision_svm = precision_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","recall_svm = recall_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","f1_svm = f1_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","\n","# Menampilkan hasil evaluasi\n","print(\"\\nEvaluation Metrics for SVM Model (Kaggle-Vader):\")\n","print(f\"Accuracy: {accuracy_svm:.4f}\")\n","print(f\"Precision: {precision_svm:.4f}\")\n","print(f\"Recall: {recall_svm:.4f}\")\n","print(f\"F1-score: {f1_svm:.4f}\")\n","\n","print(\"\\nClassification Report for SVM Model (Kaggle-Vader):\\n\", classification_rep_svm)\n"],"metadata":{"id":"2OGVCKAW2wpV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dc13aaea-4b47-404f-fb94-3dfdcbbe93b0","executionInfo":{"status":"ok","timestamp":1711183927753,"user_tz":-420,"elapsed":47032,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics for SVM Model (Kaggle-Vader):\n","Accuracy: 0.94\n","Precision: 0.95\n","Recall: 0.94\n","F1-score: 0.95\n","\n","Classification Report for SVM Model (Kaggle-Vader):\n","               precision    recall  f1-score   support\n","\n","    Negative       0.81      0.90      0.86       446\n","    Positive       0.98      0.95      0.97      1978\n","\n","    accuracy                           0.94      2424\n","   macro avg       0.90      0.93      0.91      2424\n","weighted avg       0.95      0.94      0.95      2424\n","\n"]}]},{"cell_type":"markdown","source":["### Labeling Blob"],"metadata":{"id":"oHHXB_aJ2A2H"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_blob_kaggle.csv'\n","blob = pd.read_csv(path)\n","\n","# Split data into features and labels\n","text_content = blob['Content']\n","sentiment_label = blob['Sentiment']\n","\n","# Split data into training and testing sets\n","text_train, text_test, sentiment_train, sentiment_test = train_test_split(text_content, sentiment_label, test_size=0.15, random_state=42)\n","\n","print(\"Jumlah data latih:\", len(text_train))\n","print(\"Jumlah data uji:\", len(text_test))"],"metadata":{"id":"TKRY__Hk2-9f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bda06b5-b3ea-482b-cc35-fcc415cc68d5","executionInfo":{"status":"ok","timestamp":1711183927754,"user_tz":-420,"elapsed":13,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah data latih: 13731\n","Jumlah data uji: 2424\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","x_train_tfidf = tfidf_vectorizer.fit_transform(text_train)\n","x_test_tfidf = tfidf_vectorizer.transform(text_test)\n","\n","print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n","print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)"],"metadata":{"id":"hh6MefwS3ATm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2354744a-71de-4bf2-95ed-47b5ba194f57","executionInfo":{"status":"ok","timestamp":1711183927754,"user_tz":-420,"elapsed":6,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train_tfidf: (13731, 7775)\n","Shape of x_test_tfidf: (2424, 7775)\n"]}]},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","\n","# Oversampling with SMOTE\n","smote = SMOTE(sampling_strategy='auto')  # Adjust strategy as needed\n","x_train_resampled, sentiment_train_resampled = smote.fit_resample(x_train_tfidf, sentiment_train)\n","\n","# Print class distributions\n","print(\"Before SMOTE:\")\n","print(sentiment_train.value_counts())\n","print(\"\\nAfter SMOTE:\")\n","print(sentiment_train_resampled.value_counts())\n"],"metadata":{"id":"rG-G-Lup3BZE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e489be5a-ec78-4fec-a547-9d1cde774280","executionInfo":{"status":"ok","timestamp":1711183929886,"user_tz":-420,"elapsed":1445,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before SMOTE:\n","Positive    11454\n","Negative     2277\n","Name: Sentiment, dtype: int64\n","\n","After SMOTE:\n","Positive    11454\n","Negative    11454\n","Name: Sentiment, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","\n","# Inisialisasi model SVM dengan parameter yang diperbarui\n","svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","# Melatih model dengan data yang telah di-oversample\n","svm_model.fit(x_train_resampled, sentiment_train_resampled)\n","\n","# Melakukan prediksi menggunakan model yang telah dilatih\n","sentiment_pred_svm = svm_model.predict(x_test_tfidf)\n","\n","# Menghitung akurasi model\n","accuracy_svm = accuracy_score(sentiment_test, sentiment_pred_svm)\n","\n","# Membuat laporan klasifikasi tanpa target names\n","classification_rep_svm = classification_report(sentiment_test, sentiment_pred_svm)\n","\n","# Menghitung metrik presisi, recall, dan F1-score\n","precision_svm = precision_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","recall_svm = recall_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","f1_svm = f1_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","\n","# Menampilkan hasil evaluasi\n","print(\"\\nEvaluation Metrics for SVM Model (Kaggle-Blob):\")\n","print(f\"Accuracy: {accuracy_svm:.4f}\")\n","print(f\"Precision: {precision_svm:.4f}\")\n","print(f\"Recall: {recall_svm:.4f}\")\n","print(f\"F1-score: {f1_svm:.4f}\")\n","\n","print(\"\\nClassification Report for SVM Model (Kaggle-Blob):\\n\", classification_rep_svm)\n"],"metadata":{"id":"eJTZkfFn3CeL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"919044e5-ba49-4358-fd98-4c36854f158c","executionInfo":{"status":"ok","timestamp":1711183963265,"user_tz":-420,"elapsed":33383,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics for SVM Model (Kaggle-Blob):\n","Accuracy: 0.97\n","Precision: 0.97\n","Recall: 0.97\n","F1-score: 0.97\n","\n","Classification Report for SVM Model (Kaggle-Blob):\n","               precision    recall  f1-score   support\n","\n","    Negative       0.88      0.92      0.90       396\n","    Positive       0.98      0.97      0.98      2028\n","\n","    accuracy                           0.97      2424\n","   macro avg       0.93      0.95      0.94      2424\n","weighted avg       0.97      0.97      0.97      2424\n","\n"]}]},{"cell_type":"markdown","source":["### Labeling BERT"],"metadata":{"id":"MXTlWJNc2BM4"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_bert_kaggle.csv'\n","bert = pd.read_csv(path)\n","\n","# Split data into features and labels\n","text_content = bert['Content']\n","sentiment_label = bert['Sentiment']\n","\n","# Split data into training and testing sets\n","text_train, text_test, sentiment_train, sentiment_test = train_test_split(text_content, sentiment_label, test_size=0.15, random_state=42)\n","\n","print(\"Jumlah data latih:\", len(text_train))\n","print(\"Jumlah data uji:\", len(text_test))"],"metadata":{"id":"Rd5B8BQF3I89","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4dca43d-f679-4339-a06a-337aa89acd95","executionInfo":{"status":"ok","timestamp":1711183963265,"user_tz":-420,"elapsed":30,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah data latih: 13731\n","Jumlah data uji: 2424\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","x_train_tfidf = tfidf_vectorizer.fit_transform(text_train)\n","x_test_tfidf = tfidf_vectorizer.transform(text_test)\n","\n","print(\"Shape of x_train_tfidf:\", x_train_tfidf.shape)\n","print(\"Shape of x_test_tfidf:\", x_test_tfidf.shape)"],"metadata":{"id":"lFWuCF-r3Kkq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf8d8d0f-316e-41bc-c903-2ba0e9eb339c","executionInfo":{"status":"ok","timestamp":1711183963265,"user_tz":-420,"elapsed":27,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train_tfidf: (13731, 7775)\n","Shape of x_test_tfidf: (2424, 7775)\n"]}]},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","\n","# Oversampling with SMOTE\n","smote = SMOTE(sampling_strategy='auto')  # Adjust strategy as needed\n","x_train_resampled, sentiment_train_resampled = smote.fit_resample(x_train_tfidf, sentiment_train)\n","\n","# Print class distributions\n","print(\"Before SMOTE:\")\n","print(sentiment_train.value_counts())\n","print(\"\\nAfter SMOTE:\")\n","print(sentiment_train_resampled.value_counts())\n"],"metadata":{"id":"cJIwx15a3LgK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"85b6be72-3e31-4fd7-ca58-df141b191782","executionInfo":{"status":"ok","timestamp":1711183969537,"user_tz":-420,"elapsed":6297,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before SMOTE:\n","Negative    8062\n","Positive    5669\n","Name: Sentiment, dtype: int64\n","\n","After SMOTE:\n","Negative    8062\n","Positive    8062\n","Name: Sentiment, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n","\n","# Inisialisasi model SVM dengan parameter yang diperbarui\n","svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","# Melatih model dengan data yang telah di-oversample\n","svm_model.fit(x_train_resampled, sentiment_train_resampled)\n","\n","# Melakukan prediksi menggunakan model yang telah dilatih\n","sentiment_pred_svm = svm_model.predict(x_test_tfidf)\n","\n","# Menghitung akurasi model\n","accuracy_svm = accuracy_score(sentiment_test, sentiment_pred_svm)\n","\n","# Membuat laporan klasifikasi tanpa target names\n","classification_rep_svm = classification_report(sentiment_test, sentiment_pred_svm)\n","\n","# Menghitung metrik presisi, recall, dan F1-score\n","precision_svm = precision_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","recall_svm = recall_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","f1_svm = f1_score(sentiment_test, sentiment_pred_svm, average=\"weighted\")\n","\n","# Menampilkan hasil evaluasi\n","print(\"\\nEvaluation Metrics for SVM Model (Kaggle-BERT):\")\n","print(f\"Accuracy: {accuracy_svm:.4f}\")\n","print(f\"Precision: {precision_svm:.4f}\")\n","print(f\"Recall: {recall_svm:.4f}\")\n","print(f\"F1-score: {f1_svm:.4f}\")\n","\n","print(\"\\nClassification Report for SVM Model (Kaggle-BERT):\\n\", classification_rep_svm)\n"],"metadata":{"id":"LFXuz26f3Mke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"988eee89-3978-4ce0-e7e7-6f468902cfa7","executionInfo":{"status":"ok","timestamp":1711183991904,"user_tz":-420,"elapsed":22388,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics for SVM Model (Kaggle-BERT):\n","Accuracy: 0.87\n","Precision: 0.87\n","Recall: 0.87\n","F1-score: 0.87\n","\n","Classification Report for SVM Model (Kaggle-BERT):\n","               precision    recall  f1-score   support\n","\n","    Negative       0.89      0.89      0.89      1404\n","    Positive       0.84      0.84      0.84      1020\n","\n","    accuracy                           0.87      2424\n","   macro avg       0.86      0.86      0.86      2424\n","weighted avg       0.87      0.87      0.87      2424\n","\n"]}]},{"cell_type":"markdown","source":["## **Evaluasi Model BERT**"],"metadata":{"id":"uU-1lYjptEUi"}},{"cell_type":"markdown","source":["### Labeling Vader\n","\n"],"metadata":{"id":"Ivg7vsZh50tF"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_vader_kaggle.csv'\n","vader = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = vader['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(vader['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"Zga40WeWALIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"],"metadata":{"id":"74SCeLxVAL5s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711184392732,"user_tz":-420,"elapsed":393443,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"a47d59b3-ddd3-42a7-909d-2e88a2373ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.28387365416558674\n","Epoch 2/3, Average Loss: 0.14521005618017774\n","Epoch 3/3, Average Loss: 0.07961704840851125\n"]}]},{"cell_type":"code","source":["# Inisialisasi vader_score sebelum loop\n","vader_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","vader_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Kaggle-Vader:\")\n","vader_score_df = pd.DataFrame(vader_score)\n","print(vader_score_df)\n"],"metadata":{"id":"wxvpE975AMnI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711184400687,"user_tz":-420,"elapsed":7962,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"e5eb34ca-00b0-486a-aac9-2fe8d2801d72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.09\n","Accuracy: 0.97\n","Precision: 0.98\n","Recall: 0.98\n","F1 Score: 0.98\n","\n","Evaluation for SVM Data Kaggle-Vader:\n","   Accuracy  Precision    Recall  F1-score\n","0  0.967409   0.984686  0.975228  0.979934\n"]}]},{"cell_type":"markdown","source":["### Labeling Blob\n","\n"],"metadata":{"id":"hPYYDi7O58T7"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_blob_kaggle.csv'\n","blob = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = blob['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(blob['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"ZptYM4nQ6Ebv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"],"metadata":{"id":"63VVo-T26bgp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711184800967,"user_tz":-420,"elapsed":392838,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"1da84f71-b77e-4907-a567-5aa8f4c7bf35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.2799467948243715\n","Epoch 2/3, Average Loss: 0.15623658416836067\n","Epoch 3/3, Average Loss: 0.09678942919275615\n"]}]},{"cell_type":"code","source":["# Inisialisasi blob_score sebelum loop\n","blob_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","blob_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Kaggle-Blob:\")\n","blob_score_df = pd.DataFrame(blob_score)\n","print(blob_score_df)\n"],"metadata":{"id":"r5LyFwn86bV-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711184808928,"user_tz":-420,"elapsed":7965,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"c6ef45b1-a8bd-46ab-e4da-94bc4294fc87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.09\n","Accuracy: 0.97\n","Precision: 0.97\n","Recall: 0.99\n","F1 Score: 0.98\n","\n","Evaluation for SVM Data Kaggle-Blob:\n","   Accuracy  Precision    Recall  F1-score\n","0  0.966997   0.973275  0.987673  0.980421\n"]}]},{"cell_type":"markdown","source":["### Labeling BERT\n","\n"],"metadata":{"id":"4PGgcrDB6BM1"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_bert_kaggle.csv'\n","bert = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","max_length = max_seq_length\n","\n","# Assuming you have data and labels defined previously\n","tokenized_texts = bert['Content'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","attention_masks = (padded_texts != 0).float()  # Create attention masks based on padded values\n","\n","# Trim or pad the sequences to max_length\n","padded_texts = padded_texts[:, :max_length]\n","attention_masks = attention_masks[:, :max_length]\n","\n","# Split the dataset into training and validation sets\n","labels = torch.tensor(bert['Sentiment'].map({'Positive': 1, 'Negative': 0}).values)\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n",")\n","\n","# Create DataLoader for training and validation sets\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"OX6cv0bn6hbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to the appropriate device\n","model.to(device)\n","\n","# Fine-tune the model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n"],"metadata":{"id":"PBcYgqMx6hTk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711185210799,"user_tz":-420,"elapsed":393632,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"ed396966-2456-45ec-cc6d-526657b5d5aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.13693502681806338\n","Epoch 2/3, Average Loss: 0.07738234363821184\n","Epoch 3/3, Average Loss: 0.054018504941446145\n"]}]},{"cell_type":"code","source":["# Inisialisasi bert_score sebelum loop\n","bert_score = []\n","\n","# Evaluate on the validation set\n","model.eval()\n","val_loss = 0\n","predictions, true_labels = [], []\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).tolist())\n","        true_labels.extend(labels.tolist())\n","\n","avg_val_loss = val_loss / len(val_dataloader)\n","\n","# Menghitung evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","# Menampilkan evaluation\n","print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","\n","# Menambahkan hasil evaluasi ke dalam list\n","bert_score.append({'Accuracy': accuracy,\n","                   'Precision': precision,\n","                   'Recall': recall,\n","                   'F1-score': f1})\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluation for SVM Data Kaggle-Bert:\")\n","bert_score_df = pd.DataFrame(bert_score)\n","print(bert_score_df)\n"],"metadata":{"id":"TBgf3JXv6hK0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711185218677,"user_tz":-420,"elapsed":7883,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"69a9afbd-f305-4477-c0f5-9fbbb65dcfe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.06\n","Accuracy: 0.98\n","Precision: 0.97\n","Recall: 0.98\n","F1 Score: 0.98\n","\n","Evaluation for SVM Data Kaggle-Bert:\n","   Accuracy  Precision    Recall  F1-score\n","0   0.97896   0.971762  0.978431  0.975085\n"]}]},{"cell_type":"markdown","source":["# **Evaluasi Aspek**"],"metadata":{"id":"0AcE1wr3YeAl"}},{"cell_type":"markdown","source":["## **Evaluasi Model SVM**"],"metadata":{"id":"aNw_V_eYaKz5"}},{"cell_type":"markdown","source":["### Labelling Vader"],"metadata":{"id":"IGNExnYaaYBb"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_vader_kaggle.csv'\n","vader = pd.read_csv(path)\n","\n","# Hitung nilai unik dari kolom aspek\n","unique_aspects = vader['Aspect'].unique()\n","\n","# Tampilkan nilai unik dari kolom aspek\n","print(\"Data Aspek dan Jumlah Sentimennya:\")\n","for aspect in unique_aspects:\n","    aspect_sentiments = vader[vader['Aspect'] == aspect]['Sentiment'].value_counts()\n","    print(f\"Aspek: {aspect}\")\n","    print(\"Sentimen  Jumlah\")\n","    for sentiment, count in aspect_sentiments.items():\n","        print(f\"{sentiment.ljust(9)} {count}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVU-V6vuaosT","executionInfo":{"status":"ok","timestamp":1711185218677,"user_tz":-420,"elapsed":21,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"e2a032cf-f09b-45ec-ce87-c36b97d52819"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Aspek dan Jumlah Sentimennya:\n","Aspek: Task Management\n","Sentimen  Jumlah\n","Positive  5647\n","Negative  1889\n","\n","Aspek: User Experience \n","Sentimen  Jumlah\n","Positive  5991\n","Negative  778\n","\n","Aspek: Monetization\n","Sentimen  Jumlah\n","Positive  1489\n","Negative  361\n","\n"]}]},{"cell_type":"code","source":["# Import modul yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","\n","# Membuat list kosong untuk menyimpan hasil evaluasi\n","vader_scores = []\n","\n","# Iterasi melalui setiap aspek unik\n","for aspect in unique_aspects:\n","    # Filter data berdasarkan aspek\n","    aspect_data = vader[vader['Aspect'] == aspect]\n","\n","    # Bagi data menjadi fitur (X) dan label sentimen (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Bagi data menjadi training dan testing set (85% train,15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Ubah data teks menjadi fitur numerik menggunakan TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # Lakukan SMOTE pada data training saja\n","    smote = SMOTE(sampling_strategy='auto')  # Sesuaikan strategi jika perlu\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n","\n","    # Buat model SVM untuk klasifikasi sentimen\n","    svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","    # Latih model SVM pada data training\n","    svm_model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Prediksi label sentimen pada data testing\n","    y_pred = svm_model.predict(X_test_tfidf)\n","\n","    # Menghitung metrik evaluasi\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    # Menambahkan hasil evaluasi ke dalam list\n","    vader_scores.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","    # Menampilkan laporan klasifikasi untuk aspek tertentu\n","    print(f\"\\nAspek: {aspect}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-score: {f1:.2f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluasi untuk Semua Aspek (Kaggle-Vader):\")\n","vader_scores = pd.DataFrame(vader_scores)\n","print(vader_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvNJi93mbIsi","executionInfo":{"status":"ok","timestamp":1711185232459,"user_tz":-420,"elapsed":13801,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"6f4802af-5945-4f14-9fa3-037b40695c6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Aspek: Task Management\n","Accuracy: 0.91\n","Precision: 0.92\n","Recall: 0.91\n","F1-score: 0.91\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.81      0.86      0.83       284\n","    Positive       0.95      0.93      0.94       847\n","\n","    accuracy                           0.91      1131\n","   macro avg       0.88      0.89      0.89      1131\n","weighted avg       0.92      0.91      0.91      1131\n","\n","\n","Aspek: User Experience \n","Accuracy: 0.96\n","Precision: 0.96\n","Recall: 0.96\n","F1-score: 0.96\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.81      0.86      0.83       111\n","    Positive       0.98      0.97      0.98       905\n","\n","    accuracy                           0.96      1016\n","   macro avg       0.89      0.92      0.90      1016\n","weighted avg       0.96      0.96      0.96      1016\n","\n","\n","Aspek: Monetization\n","Accuracy: 0.92\n","Precision: 0.93\n","Recall: 0.92\n","F1-score: 0.93\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.78      0.85      0.81        53\n","    Positive       0.96      0.94      0.95       225\n","\n","    accuracy                           0.92       278\n","   macro avg       0.87      0.90      0.88       278\n","weighted avg       0.93      0.92      0.93       278\n","\n","\n","Evaluasi untuk Semua Aspek (Kaggle-Vader):\n","             Aspect  Accuracy  Precision    Recall  F1-score\n","0   Task Management  0.914235   0.916066  0.914235  0.914955\n","1  User Experience   0.961614   0.962834  0.961614  0.962126\n","2      Monetization  0.924460   0.927838  0.924460  0.925737\n"]}]},{"cell_type":"markdown","source":["### Labelling Blob"],"metadata":{"id":"8_4NHEFpaa7r"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)n/Data Kaggle/label_sentiment_blob_kaggle.csv'\n","blob = pd.read_csv(path)\n","\n","# Hitung nilai unik dari kolom aspek\n","unique_aspects = blob['Aspect'].unique()\n","\n","# Tampilkan nilai unik dari kolom aspek\n","print(\"Data Aspek dan Jumlah Sentimennya:\")\n","for aspect in unique_aspects:\n","    aspect_sentiments = blob[blob['Aspect'] == aspect]['Sentiment'].value_counts()\n","    print(f\"Aspek: {aspect}\")\n","    print(\"Sentimen  Jumlah\")\n","    for sentiment, count in aspect_sentiments.items():\n","        print(f\"{sentiment.ljust(9)} {count}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugg-lfPuapDC","executionInfo":{"status":"ok","timestamp":1711185233044,"user_tz":-420,"elapsed":604,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"5fa6acfd-9fb1-4959-a776-ed3dea158361"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Aspek dan Jumlah Sentimennya:\n","Aspek: Task Management\n","Sentimen  Jumlah\n","Positive  5955\n","Negative  1581\n","\n","Aspek: User Experience \n","Sentimen  Jumlah\n","Positive  5954\n","Negative  815\n","\n","Aspek: Monetization\n","Sentimen  Jumlah\n","Positive  1573\n","Negative  277\n","\n"]}]},{"cell_type":"code","source":["# Import modul yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","\n","# Membuat list kosong untuk menyimpan hasil evaluasi\n","blob_scores = []\n","\n","# Iterasi melalui setiap aspek unik\n","for aspect in unique_aspects:\n","    # Filter data berdasarkan aspek\n","    aspect_data = blob[blob['Aspect'] == aspect]\n","\n","    # Bagi data menjadi fitur (X) dan label sentimen (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Bagi data menjadi training dan testing set (85% train,15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Ubah data teks menjadi fitur numerik menggunakan TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # Lakukan SMOTE pada data training saja\n","    smote = SMOTE(sampling_strategy='auto')  # Sesuaikan strategi jika perlu\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n","\n","    # Buat model SVM untuk klasifikasi sentimen\n","    svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","    # Latih model SVM pada data training\n","    svm_model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Prediksi label sentimen pada data testing\n","    y_pred = svm_model.predict(X_test_tfidf)\n","\n","    # Menghitung metrik evaluasi\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    # Menambahkan hasil evaluasi ke dalam list\n","    blob_scores.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","    # Menampilkan laporan klasifikasi untuk aspek tertentu\n","    print(f\"\\nAspek: {aspect}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-score: {f1:.2f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluasi untuk Semua Aspek (Kaggle-Blob):\")\n","blob_scores = pd.DataFrame(blob_scores)\n","print(blob_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4FtlUnDbPQr","executionInfo":{"status":"ok","timestamp":1711185247437,"user_tz":-420,"elapsed":14395,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"149abd97-a6c3-4ac4-e2c4-c5fe738f507c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Aspek: Task Management\n","Accuracy: 0.94\n","Precision: 0.94\n","Recall: 0.94\n","F1-score: 0.94\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.86      0.86      0.86       237\n","    Positive       0.96      0.96      0.96       894\n","\n","    accuracy                           0.94      1131\n","   macro avg       0.91      0.91      0.91      1131\n","weighted avg       0.94      0.94      0.94      1131\n","\n","\n","Aspek: User Experience \n","Accuracy: 0.96\n","Precision: 0.96\n","Recall: 0.96\n","F1-score: 0.96\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.78      0.92      0.84       119\n","    Positive       0.99      0.97      0.98       897\n","\n","    accuracy                           0.96      1016\n","   macro avg       0.89      0.94      0.91      1016\n","weighted avg       0.96      0.96      0.96      1016\n","\n","\n","Aspek: Monetization\n","Accuracy: 0.92\n","Precision: 0.92\n","Recall: 0.92\n","F1-score: 0.92\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.80      0.73      0.76        48\n","    Positive       0.94      0.96      0.95       230\n","\n","    accuracy                           0.92       278\n","   macro avg       0.87      0.85      0.86       278\n","weighted avg       0.92      0.92      0.92       278\n","\n","\n","Evaluasi untuk Semua Aspek (Kaggle-Blob):\n","             Aspect  Accuracy  Precision    Recall  F1-score\n","0   Task Management  0.941645   0.941471  0.941645  0.941554\n","1  User Experience   0.960630   0.964654  0.960630  0.961934\n","2      Monetization  0.920863   0.918720  0.920863  0.919484\n"]}]},{"cell_type":"markdown","source":["### Labbeling BERT"],"metadata":{"id":"6pL6oR4RadH1"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_bert_kaggle.csv'\n","bert = pd.read_csv(path)\n","\n","# Hitung nilai unik dari kolom aspek\n","unique_aspects = bert['Aspect'].unique()\n","\n","# Tampilkan nilai unik dari kolom aspek\n","print(\"Data Aspek dan Jumlah Sentimennya:\")\n","for aspect in unique_aspects:\n","    aspect_sentiments = bert[bert['Aspect'] == aspect]['Sentiment'].value_counts()\n","    print(f\"Aspek: {aspect}\")\n","    print(\"Sentimen  Jumlah\")\n","    for sentiment, count in aspect_sentiments.items():\n","        print(f\"{sentiment.ljust(9)} {count}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFo6BEiJapax","executionInfo":{"status":"ok","timestamp":1711185247437,"user_tz":-420,"elapsed":9,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"dee53adc-ae1e-4395-d7f9-b7642a89c409"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Aspek dan Jumlah Sentimennya:\n","Aspek: Task Management\n","Sentimen  Jumlah\n","Negative  5134\n","Positive  2402\n","\n","Aspek: User Experience \n","Sentimen  Jumlah\n","Positive  3684\n","Negative  3085\n","\n","Aspek: Monetization\n","Sentimen  Jumlah\n","Negative  1247\n","Positive  603\n","\n"]}]},{"cell_type":"code","source":["# Import modul yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","\n","# Membuat list kosong untuk menyimpan hasil evaluasi\n","bert_scores = []\n","\n","# Iterasi melalui setiap aspek unik\n","for aspect in unique_aspects:\n","    # Filter data berdasarkan aspek\n","    aspect_data = bert[bert['Aspect'] == aspect]\n","\n","    # Bagi data menjadi fitur (X) dan label sentimen (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Bagi data menjadi training dan testing set (85% train,15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Ubah data teks menjadi fitur numerik menggunakan TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # Lakukan SMOTE pada data training saja\n","    smote = SMOTE(sampling_strategy='auto')  # Sesuaikan strategi jika perlu\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n","\n","    # Buat model SVM untuk klasifikasi sentimen\n","    svm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', random_state=None)\n","\n","    # Latih model SVM pada data training\n","    svm_model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Prediksi label sentimen pada data testing\n","    y_pred = svm_model.predict(X_test_tfidf)\n","\n","    # Menghitung metrik evaluasi\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    # Menambahkan hasil evaluasi ke dalam list\n","    bert_scores.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","    # Menampilkan laporan klasifikasi untuk aspek tertentu\n","    print(f\"\\nAspek: {aspect}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-score: {f1:.2f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Menampilkan hasil evaluasi untuk semua aspek\n","print(\"\\nEvaluasi untuk Semua Aspek (Kaggle-BERT):\")\n","bert_scores = pd.DataFrame(bert_scores)\n","print(bert_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QwtRDpObRPD","executionInfo":{"status":"ok","timestamp":1711185262346,"user_tz":-420,"elapsed":14915,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"539fc84c-712d-46c4-a231-23f8d87ee0df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Aspek: Task Management\n","Accuracy: 0.88\n","Precision: 0.88\n","Recall: 0.88\n","F1-score: 0.88\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.92      0.90      0.91       763\n","    Positive       0.80      0.83      0.81       368\n","\n","    accuracy                           0.88      1131\n","   macro avg       0.86      0.87      0.86      1131\n","weighted avg       0.88      0.88      0.88      1131\n","\n","\n","Aspek: User Experience \n","Accuracy: 0.87\n","Precision: 0.87\n","Recall: 0.87\n","F1-score: 0.87\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.85      0.86      0.86       469\n","    Positive       0.88      0.87      0.88       547\n","\n","    accuracy                           0.87      1016\n","   macro avg       0.87      0.87      0.87      1016\n","weighted avg       0.87      0.87      0.87      1016\n","\n","\n","Aspek: Monetization\n","Accuracy: 0.90\n","Precision: 0.90\n","Recall: 0.90\n","F1-score: 0.90\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.92      0.92      0.92       186\n","    Positive       0.84      0.85      0.84        92\n","\n","    accuracy                           0.90       278\n","   macro avg       0.88      0.88      0.88       278\n","weighted avg       0.90      0.90      0.90       278\n","\n","\n","Evaluasi untuk Semua Aspek (Kaggle-BERT):\n","             Aspect  Accuracy  Precision    Recall  F1-score\n","0   Task Management  0.876216   0.878170  0.876216  0.876955\n","1  User Experience   0.866142   0.866188  0.866142  0.866162\n","2      Monetization  0.895683   0.895991  0.895683  0.895825\n"]}]},{"cell_type":"markdown","source":["## **Evaluasi Model BERT**"],"metadata":{"id":"I0iYrwPUaf2S"}},{"cell_type":"markdown","source":["### Labelling Vader"],"metadata":{"id":"BfI2PByKaiBK"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_vader_kaggle.csv'\n","vader = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an empty list to store evaluation results\n","vader_score = []\n","\n","# Iterate through each unique aspect\n","for aspect in vader['Aspect'].unique():\n","    # Filter data based on aspect\n","    aspect_data = vader[vader['Aspect'] == aspect]\n","\n","    # Split data into features (X) and sentiment labels (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Split data into training and testing sets (85% train, 15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","    max_length = max_seq_length\n","    tokenized_texts = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","    attention_masks = (padded_texts != 0).float()  # Create attention masks based on padding values\n","\n","    # Trim or pad the sequences to max_length\n","    padded_texts = padded_texts[:, :max_length]\n","    attention_masks = attention_masks[:, :max_length]\n","\n","    # Split dataset into training and validation sets\n","    labels = torch.tensor(y.map({'Positive': 1, 'Negative': 0}).values)\n","    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","        padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n","    )\n","\n","    # Create DataLoader for training and validation sets\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Fine-tune the model\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    for epoch in range(num_train_epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","            true_labels.extend(labels.tolist())\n","    avg_val_loss = val_loss / len(val_dataloader)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","\n","    print(f\"\\nEvaluation for Aspect: {aspect}\")\n","    print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","    # Append evaluation results to the list\n","    vader_score.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","# Display evaluation results for all aspects\n","print(\"\\nEvaluation for All Aspects (Kaggle-Vader):\")\n","vader_score = pd.DataFrame(vader_score)\n","print(vader_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-mY9Q9oYhf8","executionInfo":{"status":"ok","timestamp":1711185671635,"user_tz":-420,"elapsed":409308,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"93461995-3219-440c-901b-dd28c5075437"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.40883807989940124\n","Epoch 2/3, Average Loss: 0.23098743559598034\n","Epoch 3/3, Average Loss: 0.136456087148234\n","\n","Evaluation for Aspect: Task Management\n","Validation Loss: 0.15\n","Accuracy: 0.94\n","Precision: 0.98\n","Recall: 0.95\n","F1 Score: 0.96\n","Epoch 1/3, Average Loss: 0.1343271395450251\n","Epoch 2/3, Average Loss: 0.06875062531584666\n","Epoch 3/3, Average Loss: 0.03966784768789593\n","\n","Evaluation for Aspect: User Experience \n","Validation Loss: 0.09\n","Accuracy: 0.98\n","Precision: 0.98\n","Recall: 0.99\n","F1 Score: 0.99\n","Epoch 1/3, Average Loss: 0.21899502098560333\n","Epoch 2/3, Average Loss: 0.12764021135866643\n","Epoch 3/3, Average Loss: 0.07539719450287521\n","\n","Evaluation for Aspect: Monetization\n","Validation Loss: 0.12\n","Accuracy: 0.96\n","Precision: 0.97\n","Recall: 0.99\n","F1 Score: 0.98\n","\n","Evaluation for All Aspects (Kaggle-Vader):\n","             Aspect  Accuracy  Precision    Recall  F1-score\n","0   Task Management  0.944297   0.978049  0.946871  0.962208\n","1  User Experience   0.975394   0.982456  0.990055  0.986241\n","2      Monetization  0.964029   0.969432  0.986667  0.977974\n"]}]},{"cell_type":"markdown","source":["### Labelling Blob"],"metadata":{"id":"5lnshAuuaxcC"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_blob_kaggle.csv'\n","blob = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an empty list to store evaluation results\n","blob_score = []\n","\n","# Iterate through each unique aspect\n","for aspect in blob['Aspect'].unique():\n","    # Filter data based on aspect\n","    aspect_data = blob[blob['Aspect'] == aspect]\n","\n","    # Split data into features (X) and sentiment labels (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Split data into training and testing sets (85% train, 15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","    max_length = max_seq_length\n","    tokenized_texts = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","    attention_masks = (padded_texts != 0).float()  # Create attention masks based on padding values\n","\n","    # Trim or pad the sequences to max_length\n","    padded_texts = padded_texts[:, :max_length]\n","    attention_masks = attention_masks[:, :max_length]\n","\n","    # Split dataset into training and validation sets\n","    labels = torch.tensor(y.map({'Positive': 1, 'Negative': 0}).values)\n","    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","        padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n","    )\n","\n","    # Create DataLoader for training and validation sets\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Fine-tune the model\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    for epoch in range(num_train_epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","            true_labels.extend(labels.tolist())\n","    avg_val_loss = val_loss / len(val_dataloader)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","\n","    print(f\"\\nEvaluation for Aspect: {aspect}\")\n","    print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","    # Append evaluation results to the list\n","    blob_score.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","# Display evaluation results for all aspects\n","print(\"\\nEvaluation for All Aspects (Kaggle-Blob):\")\n","blob_score = pd.DataFrame(blob_score)\n","print(blob_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3189hs4axKx","executionInfo":{"status":"ok","timestamp":1711186080359,"user_tz":-420,"elapsed":408727,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"2ceac9cd-ca00-434d-9438-40a4e252c94a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.4144033778514435\n","Epoch 2/3, Average Loss: 0.2260022362629276\n","Epoch 3/3, Average Loss: 0.14885664900848225\n","\n","Evaluation for Aspect: Task Management\n","Validation Loss: 0.15\n","Accuracy: 0.94\n","Precision: 0.97\n","Recall: 0.96\n","F1 Score: 0.96\n","Epoch 1/3, Average Loss: 0.15432224505477482\n","Epoch 2/3, Average Loss: 0.09775592825996379\n","Epoch 3/3, Average Loss: 0.058869123517069966\n","\n","Evaluation for Aspect: User Experience \n","Validation Loss: 0.13\n","Accuracy: 0.97\n","Precision: 0.99\n","Recall: 0.97\n","F1 Score: 0.98\n","Epoch 1/3, Average Loss: 0.20886472061276437\n","Epoch 2/3, Average Loss: 0.12079682487994432\n","Epoch 3/3, Average Loss: 0.07858242129907012\n","\n","Evaluation for Aspect: Monetization\n","Validation Loss: 0.11\n","Accuracy: 0.96\n","Precision: 0.97\n","Recall: 0.98\n","F1 Score: 0.98\n","\n","Evaluation for All Aspects (Kaggle-Blob):\n","             Aspect  Accuracy  Precision    Recall  F1-score\n","0   Task Management  0.940760   0.966178  0.958613  0.962381\n","1  User Experience   0.965551   0.988662  0.972129  0.980326\n","2      Monetization  0.960432   0.974026  0.978261  0.976139\n"]}]},{"cell_type":"markdown","source":["### Labelling BERT"],"metadata":{"id":"E_5qBF9Pazf6"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Load the dataset\n","path = '/content/drive/MyDrive/Colab Notebooks/Resha Ananda Rahman (ABSA)/Data Kaggle/label_sentiment_bert_kaggle.csv'\n","bert = pd.read_csv(path)\n","\n","# Define hyperparameters for fine-tuning\n","learning_rate = 1e-5\n","batch_size = 32\n","max_seq_length = 128\n","num_train_epochs = 3\n","\n","# Initialize DistilBERT tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# Define the device to use (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create an empty list to store evaluation results\n","bert_score = []\n","\n","# Iterate through each unique aspect\n","for aspect in bert['Aspect'].unique():\n","    # Filter data based on aspect\n","    aspect_data = bert[bert['Aspect'] == aspect]\n","\n","    # Split data into features (X) and sentiment labels (y)\n","    X = aspect_data['Content']\n","    y = aspect_data['Sentiment']\n","\n","    # Split data into training and testing sets (85% train, 15% test)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n","\n","    # Tokenize input text and convert to PyTorch tensors with fixed size of max_seq_length\n","    max_length = max_seq_length\n","    tokenized_texts = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in tokenized_texts], batch_first=True, padding_value=0)\n","    attention_masks = (padded_texts != 0).float()  # Create attention masks based on padding values\n","\n","    # Trim or pad the sequences to max_length\n","    padded_texts = padded_texts[:, :max_length]\n","    attention_masks = attention_masks[:, :max_length]\n","\n","    # Split dataset into training and validation sets\n","    labels = torch.tensor(y.map({'Positive': 1, 'Negative': 0}).values)\n","    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","        padded_texts, attention_masks, labels, test_size=0.15, random_state=42\n","    )\n","\n","    # Create DataLoader for training and validation sets\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to the appropriate device\n","    model.to(device)\n","\n","    # Fine-tune the model\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","    for epoch in range(num_train_epochs):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        avg_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {avg_loss}\")\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    val_loss = 0\n","    predictions, true_labels = [], []\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).tolist())\n","            true_labels.extend(labels.tolist())\n","    avg_val_loss = val_loss / len(val_dataloader)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    precision = precision_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","\n","    print(f\"\\nEvaluation for Aspect: {aspect}\")\n","    print(f\"Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","    # Append evaluation results to the list\n","    bert_score.append({'Aspect': aspect,\n","                               'Accuracy': accuracy,\n","                               'Precision': precision,\n","                               'Recall': recall,\n","                               'F1-score': f1})\n","\n","# Display evaluation results for all aspects\n","print(\"\\nEvaluation for All Aspects Kaggle-BERT:\")\n","bert_score = pd.DataFrame(bert_score)\n","print(bert_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhD59V3qaxFB","executionInfo":{"status":"ok","timestamp":1711186489324,"user_tz":-420,"elapsed":408986,"user":{"displayName":"RESHA ANANDA RAHMAN","userId":"18232239472968781604"}},"outputId":"18f094a3-2fd2-4426-eef5-e598f026e2d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Average Loss: 0.1302599354521997\n","Epoch 2/3, Average Loss: 0.07000141991337122\n","Epoch 3/3, Average Loss: 0.046147073181786236\n","\n","Evaluation for Aspect: Task Management\n","Validation Loss: 0.09\n","Accuracy: 0.97\n","Precision: 0.93\n","Recall: 0.98\n","F1 Score: 0.95\n","Epoch 1/3, Average Loss: 0.15924090275333988\n","Epoch 2/3, Average Loss: 0.07562444019778114\n","Epoch 3/3, Average Loss: 0.0460282280575484\n","\n","Evaluation for Aspect: User Experience \n","Validation Loss: 0.07\n","Accuracy: 0.97\n","Precision: 0.96\n","Recall: 0.99\n","F1 Score: 0.97\n","Epoch 1/3, Average Loss: 0.15793716059997678\n","Epoch 2/3, Average Loss: 0.054466474037617445\n","Epoch 3/3, Average Loss: 0.027808378050103784\n","\n","Evaluation for Aspect: Monetization\n","Validation Loss: 0.06\n","Accuracy: 0.98\n","Precision: 0.98\n","Recall: 0.97\n","F1 Score: 0.97\n","\n","Evaluation for All Aspects Kaggle-BERT:\n","             Aspect  Accuracy  Precision    Recall  F1-score\n","0   Task Management  0.969054   0.925831  0.983696  0.953887\n","1  User Experience   0.972441   0.959292  0.990859  0.974820\n","2      Monetization  0.982014   0.978022  0.967391  0.972678\n"]}]}]}